{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot word detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "import torchaudio\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from types import SimpleNamespace\n",
    "import requests, torch\n",
    "import librosa, matplotlib.pyplot as plt, numpy as np, fastcore.all as fc\n",
    "from fastcore.all import noop, L\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See main task [here](https://github.com/MenshikovDmitry/TSU_AI_Course/blob/main/module_3.%20Sound%20Processing%20ang%20hot-word%20detection/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_stream(file = '../data/stream.mp3'):\n",
    "    stream_url = 'https://radio.kotah.ru/soundcheck'\n",
    "    r = requests.get(stream_url, stream=True)\n",
    "    with open(file, 'wb') as f:\n",
    "        try:\n",
    "            for block in r.iter_content(4096):\n",
    "                f.write(block)\n",
    "        except KeyboardInterrupt:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stones = Path('../data/stones')\n",
    "files = path_stones.ls()\n",
    "path = Path('../data/stream.mp3')\n",
    "sr = 16_000\n",
    "max_length_s = 1.2 # seconds\n",
    "max_l = int(max_length_s*sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, sr = librosa.load(path, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def split_into_frames(arr, f_size, stride):\n",
    "    n_frames = (arr.shape[0] - f_size)//stride + 1\n",
    "    strides = (stride*arr.dtype.itemsize, arr.dtype.itemsize)\n",
    "    return np.lib.stride_tricks.as_strided(arr, shape=(n_frames, f_size), strides=strides).copy()\n",
    "\n",
    "def split_audio(sound, sr, f_size=1, stride=1):\n",
    "    return split_into_frames(sound, int(sr*f_size), int(sr*stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = split_audio(s, sr, f_size=max_length_s, stride=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=4\n",
    "plt.plot(frames[idx])\n",
    "Audio(frames[idx], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio that says 'STONES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _ = librosa.load(files[0], sr=sr)\n",
    "plt.plot(s)\n",
    "Audio(s, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map to same lengh of `max_length_s` seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stones = L(librosa.load(f, sr=sr)[0] for f in files)\n",
    "stones = stones.map(lambda a: a[0: max_l] if a.shape[0] > max_l \n",
    "                    else np.hstack((a, np.zeros(max_l - a.shape[0], dtype=stones[0].dtype))))\n",
    "stones = np.stack(stones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def generate_with_stones(frames, stones, n=2000):\n",
    "    stone_samples = stones[np.random.choice(stones.shape[0], n)]\n",
    "    fr_samples = frames[np.random.choice(frames.shape[0], n)]\n",
    "    p = np.random.rand(n,1).astype(frames.dtype)*.5\n",
    "    return (1-p)*stone_samples+p*fr_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg, n_pos = 1500, 1500\n",
    "# create positive samples that contain key word\n",
    "xs = generate_with_stones(frames, stones, n_neg)\n",
    "ys = np.ones(n_neg)\n",
    "\n",
    "# take negative samples\n",
    "xs = np.concatenate((xs, frames[np.random.choice(frames.shape[0], n_pos)]))\n",
    "ys = np.concatenate((ys,np.zeros(n_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,(x,y) in enumerate(zip(xs,ys)):\n",
    "#     torchaudio.save(f'../data/{int(y)}/{i}.wav', tensor(x[None]), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create basic dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def merge_items(xs, ys):\n",
    "    return [(x,y) for x,y in zip(xs,ys)]\n",
    "\n",
    "class TfmDataset:\n",
    "    def __init__(self, items, x_tfms=None, y_tfms=None) -> None:\n",
    "        self.items = items\n",
    "        self.x_tfms = [noop] if x_tfms is None else x_tfms\n",
    "        self.y_tfms = [noop] if y_tfms is None else y_tfms\n",
    "    \n",
    "    def __len__(self): return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.items[idx]\n",
    "        for t in self.x_tfms: x=t(x)\n",
    "        for t in self.y_tfms: y=t(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def random_split_dataset(ds, split_frac=0.8):\n",
    "    '''shuffle and split dataset'''\n",
    "    split_size = int(len(ds) * split_frac)\n",
    "    indices = torch.randperm(len(ds))\n",
    "    train_indices = indices[:split_size]\n",
    "    val_indices = indices[split_size:]\n",
    "    train_subset = Subset(ds, train_indices)\n",
    "    val_subset = Subset(ds, val_indices)\n",
    "    \n",
    "    return train_subset, val_subset\n",
    "\n",
    "@fc.delegates(DataLoader.__init__)\n",
    "def dataloaders(train_ds, test_ds, **kwargs):\n",
    "    return SimpleNamespace(train=DataLoader(train_ds,**kwargs), test=DataLoader(test_ds,**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "1. Create train and test dataset\n",
    "   1. Inputs are little patches of sound\n",
    "   2. Outputs are true or false depending on wether they contain 'STONES'\n",
    "   3. To augment true data we can use other audio to add noise\n",
    "2. Create dataset and dataloader to get xs and ys\n",
    "3. Try different models \n",
    "   1. CNN that has big enough windows that STONES will be detected, just need to now HOW MUCH layers, dropouts and stuff\n",
    "   2. More advanced based on lectures, dilated convolutions and stuff, but only after tried solutions above\n",
    "4. Try non ml ([ChatGPT](https://chat.openai.com/share/3a16e0eb-c8d6-43ed-91bb-bc2089ab6d25))\n",
    "5. Play with different sound processing tools"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
