{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hits.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def tcsConv(n_in, n_out, k_size=3, stride=1, dilation=1, norm=True, act=None, pointwise=True):\n",
    "    '''Time-Channel Separable Convolution'''\n",
    "    if not pointwise: conv = nn.Sequential(nn.Conv1d(n_in, n_out, k_size,stride=stride, dilation=dilation))\n",
    "    else:\n",
    "        depthwise = nn.Conv1d(n_in, n_in, k_size, groups=n_in, dilation=dilation, padding='same')\n",
    "        pointwise = nn.Conv1d(n_in, n_out, 1)\n",
    "        conv = nn.Sequential(depthwise, pointwise)\n",
    "    if norm: conv.append(nn.BatchNorm1d(n_out))\n",
    "    if act: conv.append(act)\n",
    "    return conv\n",
    "\n",
    "\n",
    "class TCSBlock(nn.Module):\n",
    "    '''Block from http://arxiv.org/abs/2004.08531'''\n",
    "\n",
    "    def __init__(self, n_in, n_out, k_size):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.k_size = k_size\n",
    "        self.tcs_conv = tcsConv(self.n_in, self.n_out, self.k_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x, residual=None):\n",
    "        x = self.tcs_conv(x)\n",
    "        if residual is not None: x = x + residual\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MainBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out, k_size, R=1):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.k_size = k_size\n",
    "        self.R = R\n",
    "\n",
    "        self.inner_blocks = nn.ModuleList([TCSBlock(self.n_in, self.n_out, self.k_size)])\n",
    "        for _ in range(R-1):\n",
    "            self.inner_blocks.append(\n",
    "                TCSBlock(self.n_out, self.n_out, self.k_size))\n",
    "        self.residual = tcsConv(self.n_in, self.n_out,\n",
    "                                1, act=None, pointwise=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.residual(x)\n",
    "        for i, block in enumerate(self.inner_blocks):\n",
    "            if (i+1) == len(self.inner_blocks):\n",
    "                x = block(x, res)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MatchBoxNetwork(nn.Module):\n",
    "    def __init__(self, n_in, C, B=3, R=2, n_classes=1, k_init=11):\n",
    "        super().__init__()\n",
    "        k_sizes = [k*2+k_init for k in range(1, B)]\n",
    "        self.prologue = tcsConv(n_in, 128, k_init, stride=2,\n",
    "                                act=nn.ReLU(), pointwise=False)\n",
    "\n",
    "        self.blocks = nn.ModuleList([MainBlock(128, C, k_sizes[0], R)])\n",
    "        for k in k_sizes[1:]:\n",
    "            self.blocks.append(MainBlock(C, C, k, R))\n",
    "\n",
    "        self.epilogue = nn.Sequential(\n",
    "            tcsConv(C, 128, k_init, dilation=2, pointwise=False),\n",
    "            tcsConv(128, 128, 1),\n",
    "            tcsConv(128, n_classes, 1),\n",
    "            nn.AdaptiveAvgPool1d(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.prologue(x)\n",
    "        for block in self.blocks:\n",
    "            z = block(z)\n",
    "        z = self.epilogue(z).squeeze()\n",
    "        return z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
